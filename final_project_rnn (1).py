# -*- coding: utf-8 -*-
"""FINAL_PROJECT_RNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hv5Lq9oIrxNBS9nvhSW3DRrnSWMrX4U_
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from xgboost import XGBRegressor
import warnings
warnings.filterwarnings("ignore")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#2Ô∏è‚É£ Load Dataset
# Load dataset
df = pd.read_csv("/content/household_power_consumption.csv")

# Clean column names
df.columns = df.columns.str.strip().str.lower()

# Convert ONLY time column to datetime
df["time"] = pd.to_datetime(df["time"], errors="coerce")

# Set time as index
df = df.set_index("time")

# Convert all values to numeric
df = df.apply(pd.to_numeric, errors="coerce")

# Handle missing values (time-based interpolation)
df = df.interpolate(method="time")




#3Ô∏è‚É£ Feature Selection & Scaling
features = [
    "global_active_power",
    "global_reactive_power",
    "voltage",
    "global_intensity",
    "sub_metering_1",
    "sub_metering_2",
    "sub_metering_3"
]

df = df[features]

scaler = StandardScaler()
scaled_data = scaler.fit_transform(df.values)
#4Ô∏è‚É£ Create Time-Series Sequences
def create_sequences(data, seq_len, target_col=0):
    X, y = [], []
    for i in range(len(data) - seq_len):
        X.append(data[i:i+seq_len])
        y.append(data[i+seq_len, target_col])
    return np.array(X), np.array(y)

SEQ_LEN = 48
X, y = create_sequences(scaled_data, SEQ_LEN)

# Train-test split
split = int(0.8 * len(X))
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]
#5Ô∏è‚É£ PyTorch Dataset
class TimeSeriesDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]
train_loader = DataLoader(
    TimeSeriesDataset(X_train, y_train),
    batch_size=64,
    shuffle=True
)

test_loader = DataLoader(
    TimeSeriesDataset(X_test, y_test),
    batch_size=64,
    shuffle=False
)
#6Ô∏è‚É£ Transformer Model (Attention)
class TransformerForecast(nn.Module):
    def __init__(self, input_dim, d_model, nhead, num_layers):
        super().__init__()

        self.embedding = nn.Linear(input_dim, d_model)

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            batch_first=True,
            dropout=0.1
        )

        self.transformer = nn.TransformerEncoder(
            encoder_layer,
            num_layers=num_layers
        )

        self.fc = nn.Linear(d_model, 1)

    def forward(self, x):
        x = self.embedding(x)
        x = self.transformer(x)
        return self.fc(x[:, -1, :]).squeeze()
model = TransformerForecast(
    input_dim=X.shape[2],
    d_model=64,
    nhead=4,
    num_layers=2
).to(device)

criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
#7Ô∏è‚É£ Train Transformer
EPOCHS = 10

for epoch in range(EPOCHS):
    model.train()
    total_loss = 0

    for xb, yb in train_loader:
        xb, yb = xb.to(device), yb.to(device)

        optimizer.zero_grad()
        preds = model(xb)
        loss = criterion(preds, yb)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch {epoch+1}/{EPOCHS} | Loss: {total_loss/len(train_loader):.4f}")
#8Ô∏è‚É£ Transformer Prediction
model.eval()
preds_dl = []

with torch.no_grad():
    for xb, _ in test_loader:
        xb = xb.to(device)
        preds = model(xb)
        preds_dl.extend(preds.cpu().numpy())

preds_dl = np.array(preds_dl)
#9Ô∏è‚É£ Baseline Model ‚Äì XGBoost
def create_lag_features(series, lags=[1,2,3,24]):
    df_lag = pd.DataFrame()
    for lag in lags:
        df_lag[f"lag_{lag}"] = series.shift(lag)
    return df_lag.dropna()

target = df["global_active_power"]
lag_df = create_lag_features(target)
y_lag = target.iloc[lag_df.index]

split_lag = int(0.8 * len(lag_df))
X_train_lag, X_test_lag = lag_df[:split_lag], lag_df[split_lag:]
y_train_lag, y_test_lag = y_lag[:split_lag], y_lag[split_lag:]

xgb = XGBRegressor(
    n_estimators=200,
    max_depth=5,
    learning_rate=0.05
)

xgb.fit(X_train_lag, y_train_lag)
preds_xgb = xgb.predict(X_test_lag)
#üîü Evaluation Metrics
def rmse(y_true, y_pred):
    return np.sqrt(mean_squared_error(y_true, y_pred))

def mase(y_true, y_pred):
    naive = np.mean(np.abs(np.diff(y_true)))
    return np.mean(np.abs(y_true - y_pred)) / naive

def smape(y_true, y_pred):
    return np.mean(
        2 * np.abs(y_pred - y_true) /
        (np.abs(y_true) + np.abs(y_pred))
    ) * 100
#1Ô∏è‚É£1Ô∏è‚É£ Inverse Scaling & Results
y_test_inv = scaler.inverse_transform(
    np.c_[y_test.reshape(-1,1), np.zeros((len(y_test),6))]
)[:,0]

preds_dl_inv = scaler.inverse_transform(
    np.c_[preds_dl.reshape(-1,1), np.zeros((len(preds_dl),6))]
)[:,0]

print("\nTRANSFORMER MODEL")
print("RMSE :", rmse(y_test_inv, preds_dl_inv))
print("MASE :", mase(y_test_inv, preds_dl_inv))
print("sMAPE:", smape(y_test_inv, preds_dl_inv))

print("\nXGBOOST BASELINE")
print("RMSE :", rmse(y_test_lag, preds_xgb))
print("MASE :", mase(y_test_lag, preds_xgb))
print("sMAPE:", smape(y_test_lag, preds_xgb))
#1Ô∏è‚É£2Ô∏è‚É£ Save Model
torch.save(model.state_dict(), "transformer_time_series_model.pth")